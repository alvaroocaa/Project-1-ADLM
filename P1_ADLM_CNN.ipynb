{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1778c",
   "metadata": {},
   "source": [
    "# Object Detection for Traffic Sign Localization and Classification\n",
    "\n",
    "by Alvaro Castillejo Arjona\n",
    "\n",
    "CNN model using transfer learning to both locate and classify traffic signs using the German Traffic Sign Recognition Benchmark (GTSRB) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299352fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, json\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import kagglehub\n",
    "import pathlib\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b69e1",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "Import dataset using kagglehub from <https://www.kaggle.com/datasets/harbhajansingh21/german-traffic-sign-dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ee7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"harbhajansingh21/german-traffic-sign-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "data_dir = pathlib.Path(path)\n",
    "\n",
    "train_dir = data_dir / \"train.p\"\n",
    "test_dir  = data_dir / \"test.p\"\n",
    "\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data['features'], data['labels']\n",
    "\n",
    "train_images, train_labels = load_data(os.path.join(path, \"train.p\"))\n",
    "valid_images, valid_labels = load_data(os.path.join(path, \"valid.p\"))\n",
    "test_images, test_labels   = load_data(os.path.join(path, \"test.p\"))\n",
    "\n",
    "sign_df = pd.read_csv(os.path.join(path, \"signname.csv\"))\n",
    "class_names = sign_df[\"SignName\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b338bd",
   "metadata": {},
   "source": [
    "## Data pre-processing & augnmentation\n",
    "\n",
    "Set constants of the model and transform data using data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "NUM_CLASSES = 43  \n",
    "CLASSES = 43      \n",
    "INPUT_SIZE = 96   \n",
    "CANVAS = 96       \n",
    "OBJ_SIZE = 72     \n",
    "BATCH_SIZE = 96   \n",
    "INPUT_SHAPE = (96, 96, 3) \n",
    "\n",
    "# Training preprocessing: augment image, resize it, randomly place it on canvas, and return one-hot label + normalized bbox\n",
    "def transform(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Data augmentaiton: Randomly adjust brightness, contrast, hue, and saturation\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "    image = tf.image.random_hue(image, max_delta=0.08) \n",
    "    image = tf.image.random_saturation(image, lower=0.6, upper=1.4)\n",
    "    \n",
    "    # Resize the sign to the defined object size\n",
    "    image = tf.image.resize(image, (OBJ_SIZE, OBJ_SIZE))\n",
    "\n",
    "    # Randomly place the sign on the canvas\n",
    "    xmin = tf.random.uniform((), 0, CANVAS - OBJ_SIZE, dtype=tf.int32)\n",
    "    ymin = tf.random.uniform((), 0, CANVAS - OBJ_SIZE, dtype=tf.int32)\n",
    "\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, CANVAS, CANVAS)\n",
    "\n",
    "    # Normalize Bbox coordinates\n",
    "    xmin_f = tf.cast(xmin, tf.float32) / CANVAS\n",
    "    ymin_f = tf.cast(ymin, tf.float32) / CANVAS\n",
    "    xmax_f = tf.cast(xmin + OBJ_SIZE, tf.float32) / CANVAS\n",
    "    ymax_f = tf.cast(ymin + OBJ_SIZE, tf.float32) / CANVAS\n",
    "    bbox = tf.stack([xmin_f, ymin_f, xmax_f, ymax_f])\n",
    "\n",
    "    return image, (tf.one_hot(label, NUM_CLASSES), bbox)\n",
    "\n",
    "# Validation preprocessing: resize image, center it on canvas, and return one-hot label + normalized bbox\n",
    "def transform_val(image, label):\n",
    "    \n",
    "    # Standardize validation to match training scale (0-255)\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    image = tf.image.resize(image, (OBJ_SIZE, OBJ_SIZE))\n",
    "\n",
    "    # Center the sign on the canvas for validation consistency\n",
    "    xmin = (CANVAS - OBJ_SIZE) // 2\n",
    "    ymin = (CANVAS - OBJ_SIZE) // 2\n",
    "\n",
    "    image = tf.image.pad_to_bounding_box(image, ymin, xmin, CANVAS, CANVAS)\n",
    "\n",
    "    # Bbox coordinates\n",
    "    xmin_f = xmin / CANVAS\n",
    "    ymin_f = ymin / CANVAS\n",
    "    xmax_f = (xmin + OBJ_SIZE) / CANVAS\n",
    "    ymax_f = (ymin + OBJ_SIZE) / CANVAS\n",
    "    bbox = tf.stack([xmin_f, ymin_f, xmax_f, ymax_f])\n",
    "\n",
    "    return image, (tf.one_hot(label, NUM_CLASSES), bbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ae682",
   "metadata": {},
   "source": [
    "Transform training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75b20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_ds = train_ds.shuffle(5000)\n",
    "train_ds = train_ds.map(transform, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((valid_images, valid_labels))\n",
    "valid_ds = valid_ds.map(transform_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.batch(256)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "test_ds = test_ds.map(transform_val, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840eb7e4",
   "metadata": {},
   "source": [
    "## Visualization Utilities\n",
    "\n",
    "These functions are used to draw bounding boxes around the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9befdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_width = 96\n",
    "im_height = 96\n",
    "use_normalized_coordinates = True\n",
    "iou_threshold = 0.5\n",
    "\n",
    "\n",
    "def draw_bounding_boxes_on_image_array(image,\n",
    "                                       boxes,\n",
    "                                       color=('red',),\n",
    "                                       thickness=2,\n",
    "                                       display_str_list=()):\n",
    "    \"\"\"\n",
    "    image: numpy array (96,96,3)\n",
    "    boxes: [N,4] in format [xmin, ymin, xmax, ymax] normalized\n",
    "    \"\"\"\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image.astype(np.uint8)).convert(\"RGB\")\n",
    "    draw = PIL.ImageDraw.Draw(image_pil)\n",
    "\n",
    "    im_width, im_height = image_pil.size\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "\n",
    "        left   = xmin * im_width\n",
    "        right  = xmax * im_width\n",
    "        top    = ymin * im_height\n",
    "        bottom = ymax * im_height\n",
    "\n",
    "        draw.rectangle(\n",
    "            [(left, top), (right, bottom)],\n",
    "            outline=color[i % len(color)],\n",
    "            width=thickness\n",
    "        )\n",
    "\n",
    "        if display_str_list:\n",
    "            draw.text((left, top - 10),\n",
    "                      display_str_list[i],\n",
    "                      fill=color[i % len(color)])\n",
    "\n",
    "    return np.array(image_pil)\n",
    "\n",
    "\n",
    "def draw_bounding_boxes_on_image(image,\n",
    "                                 boxes,\n",
    "                                 color = [],\n",
    "                                 thickness = 1,\n",
    "                                 display_str_list = ()):\n",
    "  \"\"\"Draws bounding boxes on image.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    boxes: a 2 dimensional numpy array of [N, 4]: (ymin, xmin, ymax, xmax).\n",
    "           The coordinates are in normalized format between [0, 1].\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: a list of strings for each bounding box.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if boxes is not a [N, 4] array\n",
    "  \"\"\"\n",
    "  boxes_shape = boxes.shape\n",
    "  if not boxes_shape:\n",
    "    return\n",
    "  if len(boxes_shape) != 2 or boxes_shape[1] != 4:\n",
    "    raise ValueError('Input must be of size [N, 4]')\n",
    "  for i in range(boxes_shape[0]):\n",
    "    draw_bounding_box_on_image(image, boxes[i, 1], boxes[i, 0], boxes[i, 3],\n",
    "                               boxes[i, 2], color[i], thickness, display_str_list[i])\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               color = 'red',\n",
    "                               thickness = 1,\n",
    "                               display_str = None,\n",
    "                               use_normalized_coordinates = True):\n",
    "  \"\"\"Adds a bounding box to an image.\n",
    "  Bounding box coordinates can be specified in either absolute (pixel) or\n",
    "  normalized coordinates by setting the use_normalized_coordinates argument.\n",
    "  Args:\n",
    "    image: a PIL.Image object.\n",
    "    ymin: ymin of bounding box.\n",
    "    xmin: xmin of bounding box.\n",
    "    ymax: ymax of bounding box.\n",
    "    xmax: xmax of bounding box.\n",
    "    color: color to draw bounding box. Default is red.\n",
    "    thickness: line thickness. Default value is 4.\n",
    "    display_str_list: string to display in box\n",
    "    use_normalized_coordinates: If True (default), treat coordinates\n",
    "      ymin, xmin, ymax, xmax as relative to the image.  Otherwise treat\n",
    "      coordinates as absolute.\n",
    "  \"\"\"\n",
    "  draw = PIL.ImageDraw.Draw(image)\n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "  draw.line([(left, top), (left, bottom), (right, bottom),\n",
    "             (right, top), (left, top)], width = thickness, fill = color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9583a62",
   "metadata": {},
   "source": [
    "These utilities are used to visualize the data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell contains helper functions used for visualization\n",
    "and downloads only.\n",
    "\n",
    "You can skip reading it, as there is very\n",
    "little Keras or Tensorflow related code here.\n",
    "\"\"\"\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rc('image', cmap='gray')\n",
    "plt.rc('grid', linewidth=0)\n",
    "plt.rc('xtick', top=False, bottom=False, labelsize='large')\n",
    "plt.rc('ytick', left=False, right=False, labelsize='large')\n",
    "plt.rc('axes', facecolor='F8F8F8', titlesize=\"large\", edgecolor='white')\n",
    "plt.rc('text', color='a8151a')\n",
    "plt.rc('figure', facecolor='F0F0F0')# Matplotlib fonts\n",
    "MATPLOTLIB_FONT_DIR = os.path.join(os.path.dirname(plt.__file__), \"mpl-data/fonts/ttf\")\n",
    "\n",
    "# pull a batch from the datasets. This code is not very nice, it gets much better in eager mode (TODO)\n",
    "def dataset_to_numpy_util(training_dataset, validation_dataset, N):\n",
    "\n",
    "    batch_train_ds = training_dataset.unbatch().batch(N)\n",
    "\n",
    "    for val_imgs, (val_labels, val_boxes) in validation_dataset.take(1):\n",
    "        validation_images = val_imgs.numpy()\n",
    "        validation_labels = np.argmax(val_labels.numpy(), axis=1)\n",
    "        validation_boxes  = val_boxes.numpy()\n",
    "\n",
    "    for train_imgs, (train_labels, train_boxes) in batch_train_ds.take(1):\n",
    "        training_images = train_imgs.numpy()\n",
    "        training_labels = np.argmax(train_labels.numpy(), axis=1)\n",
    "        training_boxes  = train_boxes.numpy()\n",
    "\n",
    "    return (training_images, training_labels, training_boxes,\n",
    "            validation_images, validation_labels, validation_boxes)\n",
    "\n",
    "\n",
    "# create digits from local fonts for testing\n",
    "def create_digits_from_local_fonts(n):\n",
    "  font_labels = []\n",
    "  img = PIL.Image.new('LA', (96*n, 96), color = (0,255)) # format 'LA': black in channel 0, alpha in channel 1\n",
    "  font1 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'DejaVuSansMono-Oblique.ttf'), 25)\n",
    "  font2 = PIL.ImageFont.truetype(os.path.join(MATPLOTLIB_FONT_DIR, 'STIXGeneral.ttf'), 25)\n",
    "  d = PIL.ImageDraw.Draw(img)\n",
    "  for i in range(n):\n",
    "    font_labels.append(i%10)\n",
    "    d.text((7+i*96,0 if i<10 else -4), str(i%10), fill=(255,255), font=font1 if i<10 else font2)\n",
    "  font_digits = np.array(img.getdata(), np.float32)[:,0] / 255.0 # black in channel 0, alpha in channel 1 (discarded)\n",
    "  font_digits = np.reshape(np.stack(np.split(np.reshape(font_digits, [96, 96*n]), n, axis=1), axis=0), [n, 96*96])\n",
    "  return font_digits, font_labels\n",
    "\n",
    "\n",
    "# utility to display a row of digits with their predictions\n",
    "def display_digits_with_boxes(images,\n",
    "                              predictions,\n",
    "                              labels,\n",
    "                              pred_bboxes,\n",
    "                              true_bboxes,\n",
    "                              iou,\n",
    "                              title):\n",
    "\n",
    "    n = 10\n",
    "    indexes = np.random.choice(len(images), size=n)\n",
    "\n",
    "    plt.figure(figsize=(20,4))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    for i, idx in enumerate(indexes):\n",
    "\n",
    "        ax = plt.subplot(1, n, i+1)\n",
    "\n",
    "        # convert to uint8 for drawing\n",
    "        img = images[idx].astype(np.uint8)\n",
    "\n",
    "        boxes = []\n",
    "        colors = []\n",
    "        names = []\n",
    "\n",
    "        if len(true_bboxes):\n",
    "            boxes.append(true_bboxes[idx])\n",
    "            colors.append('green')\n",
    "            names.append(\"true\")\n",
    "\n",
    "        if len(pred_bboxes):\n",
    "            boxes.append(pred_bboxes[idx])\n",
    "            colors.append('red')\n",
    "            names.append(\"pred\")\n",
    "\n",
    "        if boxes:\n",
    "            img = draw_bounding_boxes_on_image_array(\n",
    "                img,\n",
    "                np.array(boxes),\n",
    "                color=colors,\n",
    "                display_str_list=names\n",
    "            )\n",
    "\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        pred_class = predictions[idx]\n",
    "        true_class = labels[idx]\n",
    "\n",
    "        title_color = \"red\" if pred_class != true_class else \"black\"\n",
    "        ax.set_title(class_names[pred_class], color=title_color, fontsize=9)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7de74",
   "metadata": {},
   "source": [
    "### Visualize Data\n",
    "\n",
    "Visualize training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_digits, training_labels, training_bboxes,\n",
    " validation_digits, validation_labels, validation_bboxes) = dataset_to_numpy_util(train_ds, valid_ds, 10)\n",
    "\n",
    "display_digits_with_boxes(\n",
    "    training_digits,\n",
    "    training_labels,\n",
    "    training_labels,\n",
    "    np.array([]),\n",
    "    training_bboxes,\n",
    "    np.array([]),\n",
    "    \"training images and their labels\"\n",
    ")\n",
    "\n",
    "display_digits_with_boxes(\n",
    "    validation_digits,\n",
    "    validation_labels,\n",
    "    validation_labels,\n",
    "    np.array([]),\n",
    "    validation_bboxes,\n",
    "    np.array([]),\n",
    "    \"validation images and their labels\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502637f3",
   "metadata": {},
   "source": [
    "## Build transfer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955fb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model():\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        input_shape=INPUT_SHAPE,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    base_model.trainable = False \n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    \n",
    "    features = base_model(x, training=False)\n",
    "    shared = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "    \n",
    "    # Classification Head\n",
    "    class_branch = tf.keras.layers.Dense(256, activation='relu')(shared)\n",
    "    class_branch = tf.keras.layers.BatchNormalization()(class_branch)\n",
    "    class_branch = tf.keras.layers.Dropout(0.4)(class_branch) \n",
    "    class_out = tf.keras.layers.Dense(CLASSES, activation='softmax', name='classifier_head')(class_branch)\n",
    "    \n",
    "    # Regression Head\n",
    "    reg_branch = tf.keras.layers.Dense(128, activation='relu')(shared)\n",
    "    bbox_out = tf.keras.layers.Dense(4, activation='sigmoid', name='regressor_head')(reg_branch)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=[class_out, bbox_out])\n",
    "\n",
    "\n",
    "model = build_transfer_learning_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d2403",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), # Faster start\n",
    "    loss={\"classifier_head\": \"categorical_crossentropy\", \"regressor_head\": \"mse\"},\n",
    "    loss_weights={\"classifier_head\": 10.0, \"regressor_head\": 1.0},\n",
    "    metrics={\"classifier_head\": \"accuracy\", \"regressor_head\": \"mse\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b9189",
   "metadata": {},
   "source": [
    "### First training phase\n",
    "\n",
    "First training phase with MobilNetV2 layer frozen to maintain features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_classifier_head_loss', # We monitor the head you want to improve\n",
    "    factor=0.2,                         # Drop LR by 5x when it plateaus\n",
    "    patience=3,                         # Wait 3 epochs before droppi\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[lr_scheduler] # Add it here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b974a9b",
   "metadata": {},
   "source": [
    "### Second training phase, classifier head fine-tunning\n",
    "\n",
    "Second training phase with MobilNetV2 layer unfrozen from layer 80 towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_layer = next(l for l in model.layers if 'mobilenetv2' in l.name)\n",
    "base_model_layer.trainable = True\n",
    "\n",
    "for layer in base_model_layer.layers[:80]: \n",
    "    layer.trainable = False\n",
    "\n",
    "# Re-compile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6), \n",
    "    loss={\"classifier_head\": \"categorical_crossentropy\", \"regressor_head\": \"mse\"},\n",
    "    loss_weights={\"classifier_head\": 10.0, \"regressor_head\": 1.0},\n",
    "    metrics={\"classifier_head\": \"accuracy\", \"regressor_head\": \"mse\"}\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_classifier_head_accuracy', \n",
    "    patience=5, \n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790ce76",
   "metadata": {},
   "source": [
    "### Third training phase, regression head fine-tunning\n",
    "\n",
    "Third training phase with MobilNetV2 layer unfrozen from layer 80 towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a689f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the base model unfrozen from layer 80 onwards, just like Phase 2.\n",
    "# Shift the loss weights to prioritize the MSE!\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Slightly higher than 5e-6 to allow coordinate shifts\n",
    "    loss={\"classifier_head\": \"categorical_crossentropy\", \"regressor_head\": \"mse\"},\n",
    "    loss_weights={\"classifier_head\": 1.0, \"regressor_head\": 15.0}, # MASSIVE boost to bounding boxes!\n",
    "    metrics={\"classifier_head\": \"accuracy\", \"regressor_head\": \"mse\"}\n",
    ")\n",
    "\n",
    "early_stop_mse = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_regressor_head_mse', # Now we monitor the MSE!\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='min' # We want MSE to go DOWN (unlike accuracy, which we wanted to go UP)\n",
    ")\n",
    "\n",
    "history_refinement = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=20, # 15 epochs should be plenty to push it down\n",
    "    callbacks=[early_stop_mse] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84624a",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "### Regressor head MSE on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37acd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine the histories safely\n",
    "combined_history = {}\n",
    "histories = [history, history_finetune, history_refinement] \n",
    "\n",
    "# Only grab the metrics we actually want to plot\n",
    "important_metrics = ['classifier_head_accuracy', 'val_classifier_head_accuracy', \n",
    "                     'regressor_head_mse', 'val_regressor_head_mse']\n",
    "\n",
    "def plot_full_journey(combined_hist, metric_name, title, ylim=None):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    # Get combined data\n",
    "    train = combined_hist[metric_name]\n",
    "    val = combined_hist['val_' + metric_name]\n",
    "\n",
    "    # Plot the lines\n",
    "    plt.plot(train, label=\"Train\", linewidth=2, color='blue')\n",
    "    plt.plot(val, label=\"Validation\", linewidth=2, color='orange')\n",
    "\n",
    "    # Draw vertical lines to mark the phases\n",
    "    phase1_end = len(histories[0].history[metric_name]) - 1\n",
    "    phase2_end = phase1_end + len(histories[1].history[metric_name])\n",
    "\n",
    "    plt.axvline(x=phase1_end, color='red', linestyle='--', alpha=0.7, label='Phase 2: Unfreeze')\n",
    "    plt.axvline(x=phase2_end, color='green', linestyle='--', alpha=0.7, label='Phase 3: Refinement')\n",
    "\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Total Epochs\", fontsize=12)\n",
    "    plt.ylabel(metric_name.replace(\"_\", \" \").title(), fontsize=12)\n",
    "\n",
    "    if ylim:\n",
    "        plt.ylim(0, ylim)\n",
    "\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Generate the plots!\n",
    "\n",
    "for metric in important_metrics:\n",
    "    combined_history[metric] = []\n",
    "    for hist in histories:\n",
    "        # If the metric exists in this specific history phase, add it\n",
    "        if metric in hist.history:\n",
    "            combined_history[metric].extend(hist.history[metric])\n",
    "        else:\n",
    "            print(f\"Warning: {metric} not found in one of the phases. Skipping.\")\n",
    "\n",
    "# Now run the plot function as before!\n",
    "plot_full_journey(combined_history, \"classifier_head_accuracy\", \"Classification Accuracy over 3 Phases\", ylim=1.0)\n",
    "plot_full_journey(combined_history, \"regressor_head_mse\", \"Bounding Box MSE over 3 Phases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c590bab",
   "metadata": {},
   "source": [
    "### Intersection over union\n",
    "\n",
    "Calculate the I-O-U metric to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18164c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(pred_box, true_box):\n",
    "    xmin_pred, ymin_pred, xmax_pred, ymax_pred =  np.split(pred_box, 4, axis = 1)\n",
    "    xmin_true, ymin_true, xmax_true, ymax_true = np.split(true_box, 4, axis = 1)\n",
    "\n",
    "    smoothing_factor = 1e-10\n",
    "\n",
    "    xmin_overlap = np.maximum(xmin_pred, xmin_true)\n",
    "    xmax_overlap = np.minimum(xmax_pred, xmax_true)\n",
    "    ymin_overlap = np.maximum(ymin_pred, ymin_true)\n",
    "    ymax_overlap = np.minimum(ymax_pred, ymax_true)\n",
    "\n",
    "    pred_box_area = (xmax_pred - xmin_pred) * (ymax_pred - ymin_pred)\n",
    "    true_box_area = (xmax_true - xmin_true) * (ymax_true - ymin_true)\n",
    "\n",
    "    overlap_area = np.maximum((xmax_overlap - xmin_overlap), 0)  * np.maximum((ymax_overlap - ymin_overlap), 0)\n",
    "    union_area = (pred_box_area + true_box_area) - overlap_area\n",
    "\n",
    "    iou = (overlap_area + smoothing_factor) / (union_area + smoothing_factor)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc654a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(validation_digits, batch_size=96)\n",
    "predicted_labels = np.argmax(predictions[0], axis=1)\n",
    "\n",
    "predicted_bboxes = predictions[1]\n",
    "\n",
    "iou = intersection_over_union(predicted_bboxes, validation_bboxes)\n",
    "\n",
    "iou_threshold = 0.5\n",
    "\n",
    "print(\"Number of predictions where iou > threshold(%s): %s\" % (iou_threshold, (iou >= iou_threshold).sum()))\n",
    "print(\"Number of predictions where iou < threshold(%s): %s\" % (iou_threshold, (iou < iou_threshold).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89878833",
   "metadata": {},
   "source": [
    "### Visualize predictions\n",
    "The following code will make predictions and visualize both the classification and the predicted bounding boxes.\n",
    "- The true bounding box labels will be in green, and the model's predicted bounding boxes are in red.\n",
    "- The predicted number is shown below the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b1878",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_digits_with_boxes(validation_digits, predicted_labels, validation_labels, predicted_bboxes, validation_bboxes, iou, \"True and Predicted values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Evaluation\n",
    "print(\"\\n--- Performing Final Evaluation on Unseen Test Set ---\")\n",
    "test_results = model.evaluate(test_ds)\n",
    "\n",
    "# The results order: [loss, classifier_loss, regressor_loss, classifier_accuracy, regressor_mse]\n",
    "print(f\"Test Total Loss: {test_results[0]:.4f}\")\n",
    "print(f\"Test Accuracy (Classification): {test_results[3]*100:.2f}%\")\n",
    "print(f\"Test MSE (Bounding Box): {test_results[4]:.4f}\")\n",
    "\n",
    "# Visualize Predictions on the Test Set\n",
    "for test_imgs, (test_labels_oh, test_boxes) in test_ds.take(1):\n",
    "    test_digits_batch = test_imgs.numpy()\n",
    "    test_actual_labels = np.argmax(test_labels_oh.numpy(), axis=1)\n",
    "    test_actual_bboxes = test_boxes.numpy()\n",
    "\n",
    "# Run predictions on this batch\n",
    "test_predictions = model.predict(test_digits_batch)\n",
    "test_predicted_labels = np.argmax(test_predictions[0], axis=1)\n",
    "test_predicted_bboxes = test_predictions[1]\n",
    "\n",
    "# Calculate IOU (Intersection Over Union) for the test batch\n",
    "test_iou = intersection_over_union(test_predicted_bboxes, test_actual_bboxes)\n",
    "\n",
    "# Display the results\n",
    "display_digits_with_boxes(\n",
    "    test_digits_batch, \n",
    "    test_predicted_labels, \n",
    "    test_actual_labels, \n",
    "    test_predicted_bboxes, \n",
    "    test_actual_bboxes, \n",
    "    test_iou, \n",
    "    \"Final Test Set Performance (Unseen Data)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bc70a",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e85b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model (architecture, weights, and optimizer state)\n",
    "model.save('traffic_sign_model_final.keras')\n",
    "\n",
    "# Also save just the weights as a backup\n",
    "model.save_weights('traffic_sign_weights_final.weights.h5')\n",
    "\n",
    "print(\"Model saved successfully as 'traffic_sign_model_final.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
